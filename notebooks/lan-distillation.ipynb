{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a49d9b2",
   "metadata": {},
   "source": [
    "## Flow to LAN distilation\n",
    "\n",
    "__Goals:__ Combine the flexibility and training efficiency of normalizing flows with the inference efficiency of MLPs by distilling trained flow-based density estimator into a likelihood approximation network.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "701f7fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "import numpy as np\n",
    "import torch as th\n",
    "import jax.random as jrd\n",
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import arviz as az\n",
    "import pymc as pm\n",
    "\n",
    "from copy import deepcopy\n",
    "from torch import Tensor\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from sbi.analysis import pairplot\n",
    "from sbi.inference import MNLE\n",
    "from sbi.utils import BoxUniform\n",
    "\n",
    "from sbi.utils.user_input_checks import (\n",
    "    check_sbi_inputs,\n",
    "    process_prior,\n",
    "    process_simulator,\n",
    ")\n",
    "\n",
    "from lanfactory.trainers import MLPJax, ModelTrainerJaxMLP\n",
    "\n",
    "import hssm\n",
    "from hssm.config import ModelConfig\n",
    "from hssm.distribution_utils.dist import make_distribution, make_hssm_rv, make_likelihood_callable\n",
    "from hssm.utils import decorate_atomic_simulator\n",
    "\n",
    "from ssms.basic_simulators.simulator import simulator as ssm_simulator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ab7c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seeds for reproducibility\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "th.manual_seed(seed)\n",
    "key = jrd.PRNGKey(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17dc204c",
   "metadata": {},
   "source": [
    "#### Step 1: Preparing the simulator\n",
    "\n",
    "References: \n",
    "- https://github.com/lnccbrown/ssm-simulators/blob/main/notebooks/basic_tutorial.ipynb\n",
    "- https://sbi-dev.github.io/sbi/v0.23.3/tutorials/00_getting_started/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc03f94b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulator(theta: Tensor) -> Tensor:\n",
    "    \"\"\"\n",
    "    SBI expects simulator of that takes a tensor of param values [n_samples x param_dim]\n",
    "      and return tensor of observations [n_samples x obs_dim].\n",
    "    \"\"\"\n",
    "    xs = []\n",
    "    for t in tqdm(theta):\n",
    "        sim_out = ssm_simulator(\n",
    "            model=\"ddm\",\n",
    "            theta={ \"v\": t[0].item(),\n",
    "                    \"a\": t[1].item(),\n",
    "                    \"z\": t[2].item(),\n",
    "                    \"t\": t[3].item() },\n",
    "            n_samples=1,\n",
    "            smooth_unif=False,\n",
    "        )\n",
    "        rt = sim_out[\"rts\"].squeeze()\n",
    "        choice = sim_out[\"choices\"].squeeze()\n",
    "\n",
    "        # NOTE: there is some error when using [-1, 1] coding when training MNLE \n",
    "        # This should be fixed. For now, we convert to [0, 1] coding\n",
    "        choice[choice == -1] = 0\n",
    "\n",
    "        x = th.from_numpy(np.array([rt, choice]))\n",
    "        xs.append(x)\n",
    "\n",
    "    xs = th.stack(xs, dim=0).to(th.float32)\n",
    "    return xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7a92fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define prior bounds on the parameters [v, a, z, t]\n",
    "param_lower_bounds = th.tensor([-3.0, 0.3, 0.1, 0.0])\n",
    "param_upper_bounds = th.tensor([3.0, 2.5, 0.9, 2.0])\n",
    "prior = BoxUniform(low=param_lower_bounds, high=param_upper_bounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b181b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# validate simulator and prior\n",
    "prior, num_parameters, prior_returns_numpy = process_prior(prior)\n",
    "\n",
    "# Check simulator, returns PyTorch simulator able to simulate batches.\n",
    "simulator = process_simulator(simulator, prior, prior_returns_numpy)\n",
    "\n",
    "# Consistency check after making ready for sbi.\n",
    "check_sbi_inputs(simulator, prior)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4d41d3",
   "metadata": {},
   "source": [
    "#### Step 2: Train density estimator using SBI \n",
    "\n",
    "References: \n",
    "- https://sbi-dev.github.io/sbi/v0.23.3/tutorials/00_getting_started/\n",
    "- https://sbi-dev.github.io/sbi/v0.24.0/tutorials/Example_01_DecisionMakingModel/\n",
    "- https://github.com/mackelab/mnle-for-ddms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df05835d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate MNLE training data\n",
    "num_sims_train = 100000\n",
    "theta_train = prior.sample((num_sims_train,))\n",
    "x_train = simulator(theta_train)\n",
    "\n",
    "# Train MNLE \n",
    "trainer = MNLE(prior=prior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581800d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Train MNLE\n",
    "estimator = trainer.append_simulations(theta_train, x_train).train()\n",
    "\n",
    "print('\\nMNLE training time:')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ee7069",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define theta obersvation for validation\n",
    "theta_obs = th.tensor([[1.0, 1.5, 0.5, 0.3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051b992f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the likelihood by comparing emulator and simulator, for a known theta\n",
    "synthetic_data = estimator.sample(sample_shape=(1000,), condition=theta_obs)\n",
    "real_data = simulator(theta_obs.repeat(1000, 1))\n",
    "\n",
    "# Fix the shapes of the sythentic data\n",
    "synthetic_data = synthetic_data[:,0,:]\n",
    "print('real_data.shape: ', real_data.shape)\n",
    "print('synthetic data.shape: ', synthetic_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2afd84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the histograms from real and sythetic data\n",
    "plt.figure(figsize=(8, 4))\n",
    "bins = th.linspace(-10, 10, 100)\n",
    "\n",
    "synthetic_choice_mask = synthetic_data[:, 1] == 0\n",
    "real_choice_mask = real_data[:, 1] == 0\n",
    "\n",
    "plt.hist(-synthetic_data[synthetic_choice_mask, 0], bins=bins, histtype=\"step\");\n",
    "plt.hist(-real_data[real_choice_mask, 0], bins=bins, histtype=\"step\");\n",
    "plt.legend([\"MNLE\", \"simulator\"])\n",
    "plt.hist(synthetic_data[~synthetic_choice_mask, 0], bins=bins, histtype=\"step\", \n",
    "         color=\"C0\");\n",
    "plt.hist(real_data[~real_choice_mask, 0], bins=bins, histtype=\"step\", \n",
    "         color=\"C1\");\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d732fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define MCMC parameters\n",
    "mcmc_kwargs = dict(\n",
    "    num_chains=10,\n",
    "    warmup_steps=100,\n",
    "    method=\"slice_np_vectorized\",\n",
    "    init_strategy=\"proposal\",\n",
    ")\n",
    "\n",
    "# Build posterior from the trained estimator and prior.\n",
    "mnle_posterior = trainer.build_posterior(prior=prior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215197b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test parameter recovery\n",
    "n_posterior_samples = 10000\n",
    "\n",
    "# Define parameters, simulate observation\n",
    "x_obs = simulator(theta_obs)\n",
    "mnle_posterior.set_default_x(x_obs);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4a37bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Get posterior samples and MAP estimate\n",
    "samples = mnle_posterior.sample((n_posterior_samples,))\n",
    "\n",
    "print('\\nMNLE posterior sampling time: ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b00821",
   "metadata": {},
   "outputs": [],
   "source": [
    "map = mnle_posterior.map().squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b524bae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate pair plot of posterior along with true and MAP parameters\n",
    "lower_bounds = th.tensor([-3.5, 0, 0, -.5])\n",
    "upper_bounds = th.tensor([3.5, 3, 1, 2.5])\n",
    "\n",
    "fig, ax = pairplot(samples,\n",
    "             limits=list(zip(lower_bounds, upper_bounds)),\n",
    "             figsize=(4, 4),\n",
    "             labels=[r\"$v$\", r\"$a$\", r\"$z$\", r\"$t$\"], \n",
    "             points=[theta_obs, map],\n",
    "             );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a436e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now plot pairplot with contours (obtained via KDE on the samples).\n",
    "fig, ax = pairplot([\n",
    "    prior.sample((10000,)),\n",
    "    samples,\n",
    "], \n",
    "    figsize=(6, 6),\n",
    "    diag=\"kde\",\n",
    "    upper=\"contour\", \n",
    "    kde_offdiag=dict(bins=50),\n",
    "    kde_diag=dict(bins=100),\n",
    "    contour_offdiag=dict(levels=[0.95]),\n",
    "    labels=[r\"$v$\", r\"$a$\", r\"$z$\", r\"$t$\"], \n",
    "    points=[theta_obs, map],\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f9f6ca",
   "metadata": {},
   "source": [
    "#### Step 3: Distill flow into LAN\n",
    "\n",
    "References:\n",
    "- https://github.com/lnccbrown/LANfactory\n",
    "- https://github.com/lnccbrown/LANfactory/blob/main/notebooks/basic_tutorial_jax_lan.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0432c9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(theta_train.shape)\n",
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870ac630",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get log prob evaluation for all the training data. These are the labels for the distillation.\n",
    "with th.no_grad():\n",
    "    log_p = estimator.log_prob(x_train.unsqueeze(0), condition=theta_train).squeeze(0)\n",
    "\n",
    "# Convert back to [-1, 1] coding for LAN training\n",
    "x_train_lan = deepcopy(x_train) \n",
    "x_train_lan[:, 1][x_train_lan[:, 1] == 0] = -1\n",
    "\n",
    "# LANs learn the function f: [x, theta] -> log p(x | theta).\n",
    "data = th.concat((theta_train, x_train_lan), dim=1)\n",
    "labels = log_p.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f98588bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5fbfe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build DataLoaders\n",
    "batch_size = 128\n",
    "ds = TensorDataset(data, labels)\n",
    "train_dl = DataLoader(ds, batch_size=batch_size, shuffle=True)\n",
    "valid_dl = DataLoader(ds, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Tell the trainer how big each input is\n",
    "train_dl.dataset.input_dim = data.shape[1]\n",
    "valid_dl.dataset.input_dim = data.shape[1]\n",
    "\n",
    "# Dummy placeholders so that .train_and_evaluate doesnâ€™t crash on saving\n",
    "train_dl.dataset.data_generator_config = {}\n",
    "valid_dl.dataset.data_generator_config = {}\n",
    "train_dl.dataset.file_ids = []\n",
    "valid_dl.dataset.file_ids = []\n",
    "\n",
    "\n",
    "# Train jax MLP with lanfactory\n",
    "train_config = {\"n_epochs\": 50, \"loss\": \"mse\"}\n",
    "\n",
    "mlp = MLPJax(\n",
    "    layer_sizes=(64,64,1), \n",
    "    activations = (\"relu\",\"relu\",\"linear\"), \n",
    "    train=True, \n",
    "    train_output_type=\"logprob\"\n",
    ")\n",
    "\n",
    "trainer = ModelTrainerJaxMLP(\n",
    "    train_config, \n",
    "    mlp, \n",
    "    train_dl, \n",
    "    valid_dl, \n",
    "    seed=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9abf5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Train LAN\n",
    "final_state = trainer.train_and_evaluate(    \n",
    "    output_folder=\"./jax_mlp_runs\",\n",
    "    output_file_id=\"lan_mlp\",\n",
    "    run_id=\"run1\",\n",
    "    wandb_on=False,\n",
    "    save_all=False,\n",
    ")\n",
    "\n",
    "print('\\nLAN training time: ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb712386",
   "metadata": {},
   "source": [
    "#### Step 4: posterior inference using HSSM\n",
    "\n",
    "References: \n",
    "- https://github.com/lnccbrown/HSSM\n",
    "- https://lnccbrown.github.io/HSSM/tutorials/jax_callable_contribution_onnx_example/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264cc43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "jax_logp, _ = mlp.make_forward_partial(\n",
    "    seed=seed,\n",
    "    input_dim=4 + 2,  # n-parameters (v,a,z,t) + n-data (rts and choices)\n",
    "    state=\"./jax_mlp_runs/run1_lan_lan_mlp__train_state.jax\",\n",
    "    add_jitted=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ba5b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the signature of the JAX function 1\n",
    "n_trials = 10\n",
    "jax_logp(np.tile(np.array([1.0, 1.5, 0.5, 0.3, 1.6, 1.0]), (n_trials, 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66bf8db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the signature of the JAX function 2\n",
    "n_dim_model_parameters = 4\n",
    "n_dim_data = 2\n",
    "in_ = jnp.zeros((n_trials, n_dim_model_parameters + n_dim_data))\n",
    "out = jax_logp(in_)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fddfe7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sim_wrapper(simulator_fun, theta, model, n_samples, random_state, **kwargs):\n",
    "    \"\"\" Wrap a simulator function to match HSSM's expected interface. \"\"\"\n",
    "    out = simulator_fun(\n",
    "        theta=theta,\n",
    "        model=model,\n",
    "        n_samples=n_samples,\n",
    "        random_state=random_state,\n",
    "        **kwargs,\n",
    "    )\n",
    "    return np.column_stack([out[\"rts\"], out[\"choices\"]])\n",
    "\n",
    "my_wrapped_simulator = partial(\n",
    "    sim_wrapper, simulator_fun=ssm_simulator, model=\"ddm\", n_samples=1\n",
    ")\n",
    "\n",
    "decorated_simulator = decorate_atomic_simulator(\n",
    "    model_name=\"ddm\", choices=[-1, 1], obs_dim=2\n",
    ")(my_wrapped_simulator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4bb29f",
   "metadata": {},
   "outputs": [],
   "source": [
    "decorated_simulator(\n",
    "    theta=np.tile(np.array([1.0, 1.5, 0.5, 0.3]), (10, 1)), random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e302799",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a pytensor RandomVariable\n",
    "CustomRV = make_hssm_rv(\n",
    "    simulator_fun=decorated_simulator, list_params=[\"v\", \"a\", \"z\", \"t\"]\n",
    ")\n",
    "\n",
    "# Define a likelihood function\n",
    "logp_jax_op = make_likelihood_callable(\n",
    "    loglik=jax_logp,\n",
    "    loglik_kind=\"approx_differentiable\",\n",
    "    backend=\"jax\",\n",
    "    params_is_reg=[False, False, False, False],\n",
    "    params_only=False,\n",
    ")\n",
    "\n",
    "# Define a distribution\n",
    "CustomDistribution = make_distribution(\n",
    "    rv=CustomRV,\n",
    "    loglik=logp_jax_op,\n",
    "    list_params=[\"v\", \"a\", \"z\", \"t\"],\n",
    "    bounds=dict(v=(-3, 3), a=(0.5, 3.0), z=(0.1, 0.9), t=(0, 2.0)),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01becec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "mcmc_kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d0a6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulate some data from the model\n",
    "obs_ddm = hssm.simulate_data(\n",
    "    theta = dict(\n",
    "            v=theta_obs[0][0].item(), \n",
    "            a=theta_obs[0][1].item(),     \n",
    "            t=theta_obs[0][2].item(), \n",
    "            z=theta_obs[0][3].item()\n",
    "    ), \n",
    "    model=\"ddm\", \n",
    "    size=10\n",
    ")\n",
    "\n",
    "# Test via basic pymc model\n",
    "with pm.Model() as model:\n",
    "    v = pm.Normal(\"v\", mu=0, sigma=1)\n",
    "    a = pm.Uniform(\"a\", lower=0.5, upper=3.0)\n",
    "    z = pm.Beta(\"z\", alpha=10, beta=10)\n",
    "    t = pm.Weibull(\"t\", alpha=0.5, beta=1.0)\n",
    "    CustomDistribution(\"custom\", v=v, a=a, z=z, t=t, observed=obs_ddm.values)\n",
    "    \n",
    "with model:\n",
    "    idata = pm.sample(draws=n_posterior_samples, tune=100, chains=10, nuts_sampler=\"numpyro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f1d10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_trace(idata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2842ef5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model config\n",
    "my_custom_model_config = ModelConfig(\n",
    "    response=[\"rt\", \"response\"],\n",
    "    list_params=[\"v\", \"a\", \"z\", \"t\"],\n",
    "    bounds={\n",
    "        \"v\": (-2.5, 2.5),\n",
    "        \"a\": (1.0, 3.0),\n",
    "        \"z\": (0.0, 0.9),\n",
    "        \"t\": (0.001, 2),\n",
    "    },\n",
    "    rv=decorated_simulator,\n",
    "    backend=\"jax\",\n",
    "    choices=[-1, 1],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c5065c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the HSSM model\n",
    "model = hssm.HSSM(\n",
    "    data=obs_ddm,\n",
    "    model=\"lan_distillation\",  \n",
    "    model_config=my_custom_model_config,\n",
    "    loglik_kind=\"approx_differentiable\",  # use the blackbox loglik\n",
    "    loglik=jax_logp,\n",
    "    p_outlier=0,\n",
    ")\n",
    "\n",
    "model.graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42129e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Test sampling\n",
    "model.sample(draws=n_posterior_samples, tune=100, chains=10, nuts_sampler=\"numpyro\", discard_tuned_samples=False)\n",
    "\n",
    "print('\\nHSSM sampling time: ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eadbe2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_trace(model.traces)\n",
    "plt.tight_layout()\n",
    "\n",
    "az.plot_pair(model.traces)\n",
    "plt.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lans",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
